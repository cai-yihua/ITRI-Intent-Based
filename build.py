from __future__ import annotations
import os, sys, json, time, subprocess, requests, logging, mimetypes
from pathlib import Path
from datetime import datetime
from contextlib import contextmanager
from dotenv import load_dotenv, set_key
from typing import Callable, List, Tuple, TypedDict
from tenacity import retry, stop_after_attempt, wait_fixed
import docker
from docker.errors import NotFound, APIError
from concurrent.futures import ThreadPoolExecutor, as_completed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ—¥èªŒè·¯å¾‘èˆ‡æª”å â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOG_DIR = Path("log")
LOG_DIR.mkdir(exist_ok=True)

_NOW_STR = datetime.now().strftime("%Y%m%d_%H%M")
LOG_FILE = LOG_DIR / f"{_NOW_STR}.log"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Logging è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s| %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ],
)

def log_error(msg: str):
    logging.error(msg)
    raise RuntimeError(msg)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å…±ç”¨å·¥å…· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
dotenv_path = os.path.abspath("./Backend/.env")
load_dotenv(dotenv_path=dotenv_path, override=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

dotenv_path = os.path.abspath(".env")
load_dotenv(dotenv_path=dotenv_path, override=True)

SUDO_PASSWORD = os.getenv("SUDO_PASSWORD")
N8N_EXIST = os.getenv("N8N_EXIST")

# n8n
N8N_EMAIL = os.getenv("N8N_EMAIL")
N8N_PASSWORD = os.getenv("N8N_PASSWORD")
N8N_FIRSTNAME = os.getenv("N8N_FIRSTNAME")
N8N_LASTNAME = os.getenv("N8N_LASTNAME")
N8N_BASE_URL = os.getenv("N8N_BASE_URL")
N8N_SETUP_URL = os.getenv("N8N_SETUP_URL")
N8N_LOGIN_URL = os.getenv("N8N_LOGIN_URL")
N8N_SURVEY_URL = os.getenv("N8N_SURVEY_URL")
N8N_GET_API_URL = os.getenv("N8N_GET_API_URL")
N8N_API_URL = os.getenv("N8N_API_URL")

# dify
DIFY_TAG = os.getenv("DIFY_TAG")
DIFY_EMAIL = os.getenv("DIFY_EMAIL")
DIFY_NAME = os.getenv("DIFY_NAME")
DIFY_PASSWORD = os.getenv("DIFY_PASSWORD")
DIFY_SETUP_URL = os.getenv("DIFY_SETUP_URL")
DIFY_LOGIN_URL = os.getenv("DIFY_LOGIN_URL")
DIFY_IMPORT_URL = os.getenv("DIFY_IMPORT_URL")
DIFY_BRANCH = os.getenv("DIFY_BRANCH")
API_KEY_BASE = os.getenv("API_KEY_BASE")

DIFY_ADD_MODELS_VENDOR = os.getenv("DIFY_ADD_MODELS_VENDOR")
DIFY_SET_OPENAI_API_KEY = os.getenv("DIFY_SET_OPENAI_API_KEY")
DIFY_UPLOAD_TXT = os.getenv("DIFY_UPLOAD_TXT")
DIFY_INIT_DB = os.getenv("DIFY_INIT_DB")
DIFY_BASE = os.getenv("DIFY_BASE")

DIFY_CONTAINERS: List[str] = [
    "docker-nginx-1",
    "docker-worker-1",
    "docker-api-1",
    "docker-ssrf_proxy-1",
    "docker-weaviate-1",
    "docker-sandbox-1",
    "docker-web-1",
]

DIFY_CONTAINERS_HEALTHY: List[str] = [
    "docker-db-1",
    "docker-redis-1",
    "docker-sandbox-1",
]

BACKEND_CONTAINERS: List[str] = [
    "itri-intent-backend",
    "intent-postgres-db",
    "intent-redis-db",
    "intent-pgadmin"
]

class JSONPayload(TypedDict):
    mode: str
    json_payload: dict[str]

class YamlPayload(TypedDict):
    mode: str
    yaml_content: str

@contextmanager
def step_timer(label: str):
    """
    è‡ªå‹•æŠŠèŠ±è²»æ™‚é–“å¯«é€² ./log/â€¦ å…§
    """
    logging.info(f"ğŸš© é–‹å§‹ - {label}")
    start = time.perf_counter()
    try:
        yield
    finally:
        elapsed = time.perf_counter() - start
        logging.info(f"ğŸ çµæŸ - {label}ï¼Œè€—æ™‚ {elapsed:,.2f} s")

@retry(stop=stop_after_attempt(6), wait=wait_fixed(10))
def _run_with_retry(fn, *args, **kwargs):
    return fn(*args, **kwargs)

def run_shell_script(script_name):
    try:
        subprocess.run(["chmod", "+x", script_name], check=True)
        subprocess.run(["./" + script_name], check=True, text=True, timeout=500)
        logging.info(f"âœ… {script_name} åŸ·è¡ŒæˆåŠŸ")
    except Exception as e:
        log_error(f"â— æœªé æœŸéŒ¯èª¤ï¼š{e}")

def wait_for_container_ready(containers: List[str], timeout: int = 50, require_healthy: bool = False):
    """
    ç­‰å¾…ä¸€å€‹æˆ–å¤šå€‹ container å…¨æ•¸é€²å…¥å°±ç·’ç‹€æ…‹ã€‚

    Args:
        containers      : container åç¨±åˆ—è¡¨ã€‚
        timeout         : æ¯å€‹ container æœ€å¤šç­‰å¾…ç§’æ•¸ã€‚
        require_healthy : è‹¥ image æœ‰ HEALTHCHECKï¼Œæ˜¯å¦å¿…é ˆç­‰åˆ° healthyã€‚

    Returns:
        bool: å…¨éƒ¨å°±ç·’â†’Trueï¼›ä»»ä½•ä¸€å€‹é€¾æ™‚/å¤±æ•—â†’Falseã€‚
    """
    client = docker.from_env()

    for name in containers:
        logging.info(f"â³ '{name:25}' container å°±ç·’ä¸­")

        for _ in range(timeout):
            try:
                container = client.containers.get(name)
                container.reload()
                state = container.attrs["State"]
                status = state.get("Status")                    # running / exitedâ€¦
                health = state.get("Health", {}).get("Status")  # healthy / startingâ€¦

                if status == "running" and (
                    (not require_healthy) or (health == "healthy")
                ):
                    logging.info(f"âœ… '{name:25}' container å·²å°±ç·’")
                    break                                       # é€²åˆ°ä¸‹ä¸€å€‹ container
            except NotFound:
                logging.error(f"âš ï¸ æ‰¾ä¸åˆ°åç¨±ç‚º '{name}' çš„ container")
            except APIError as err:
                logging.error(f"âš ï¸ Docker API error: {err}")

            time.sleep(1)
        else:
            # for-loop æ­£å¸¸çµæŸä»£è¡¨é€¾æ™‚
            logging.error(f"âŒ container '{name}' æœªåœ¨ {timeout}s å…§å°±ç·’")

def ensure_docker_network(network_name: str = "itri-net"):
    try:
        exists = subprocess.run(
            ["docker", "network", "inspect", network_name],
            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL
        ).returncode == 0

        if exists:
            print(f"â„¹ï¸  network å·²å­˜åœ¨ï¼Œç•¥é create")
            return

        subprocess.run(["docker", "network", "create", network_name], check=True)
        print(f"âœ… å»ºç«‹ network æˆåŠŸ")

    except subprocess.CalledProcessError as e:
        print(f"âŒ å»ºç«‹ network å¤±æ•—ï¼š{e}")
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ n8n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def n8n_setup_owner():
    """
    è¨­å®š owner è³‡è¨Š
    """    
    payload = {
        "email": N8N_EMAIL,
        "firstName": N8N_FIRSTNAME,
        "lastName": N8N_LASTNAME,
        "password": N8N_PASSWORD
    }

    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }

    try:
        response = requests.post(N8N_SETUP_URL, json=payload, headers=headers)
        response.raise_for_status()
        result = response.json()

        if result.get("data"):
            logging.info("âœ… è¨»å†ŠæˆåŠŸ")
        else:
            log_error("âŒ è¨»å†Šå¤±æ•—")

    except Exception as e:
        log_error(f"è¨»å†ŠéŒ¯èª¤ï¼š{e}")

def n8n_login() -> requests.Session:
    """
    ç™»å…¥ä¸¦å–å¾— auth_token
    """
    payload = {
        "email": N8N_EMAIL,
        "password": N8N_PASSWORD,
        "language": "zh-Hant",
        "remember_me": True
    }
    
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json"
    }
    
    session = requests.Session()

    try:
        response = session.post(N8N_LOGIN_URL, json=payload, headers=headers)
        response.raise_for_status()
        auth_token = response.cookies.get("n8n-auth")
        if auth_token:
            session.cookies.set("n8n-auth", auth_token)
            logging.info("âœ… ç™»å…¥æˆåŠŸ")
        else:
            log_error("âŒ ç™»å…¥å¤±æ•—")

        return session

    except Exception as e:
        log_error(f"ç™»å…¥éŒ¯èª¤ï¼š{e}")

def n8n_get_api_key(session) -> str:
    """
    ç²å– API KEYç²å– API KEY
    """
    payload = {
        "expiresAt": None,
        "label": "test"
    }

    try:
        response = session.post(N8N_GET_API_URL, json=payload)
        response.raise_for_status()
        result = response.json()

        if result.get("data"):
            logging.info("âœ… ç²å– API KEY æˆåŠŸ")
            api_key = result["data"].get("rawApiKey")
            return api_key
        else:
            log_error("âŒ ç²å– API KEY å¤±æ•—")

    except Exception as e:
        log_error(f"ç²å– API KEY éŒ¯èª¤ï¼š{e}")

def json_to_payload() -> List[JSONPayload]:
    """
    å°‹æ‰¾ /n8n-version/ çš„æ‰€æœ‰ JSON æª”æ¡ˆï¼Œä¸¦å°‡å…§å®¹åµŒå…¥ JSON payload ä¸­ã€‚
    """
    try:
        allowed_fields = ["name", "nodes", "connections", "settings", "staticData"]
        json_dir = os.path.join(os.getcwd(), 'n8n-version')
        payloads = []

        for filename in os.listdir(json_dir):
            if filename.endswith('.json'):
                file_path = os.path.join(json_dir, filename)
                with open(file_path, 'r', encoding='utf-8') as f:
                    json_content = json.load(f)
                    json_payload = {key: json_content[key] for key in allowed_fields if key in json_content}
                payload = {
                    "mode": "json-content",
                    "json_payload": json_payload
                }
                payloads.append(payload)

        logging.info("âœ… ç²å– JSONs æˆåŠŸ")
        return payloads

    except Exception as e:
        log_error(f"ç²å– JSON éŒ¯èª¤ï¼š{e}")
    
def n8n_create_workflow(payloads) -> List[str]:
    """
    ç™¼é€å‰µå»º workflow çš„è«‹æ±‚ï¼Œå›å‚³ workflow_id
    """
    try:
        workflow_ids = []
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "X-N8N-API-KEY": N8N_API_KEY
        }

        for payload in payloads:
            workflow_content = payload["json_payload"]

            response = requests.post(N8N_API_URL, json=workflow_content, headers=headers)
            if response.status_code == 200:
                logging.info("âœ… å‰µå»º workflow æˆåŠŸ")
                data = response.json()
                workflow_id = data.get("id")
                workflow_ids.append(workflow_id)

                # active workflow
                activate_url = f"{N8N_API_URL}/{workflow_id}/activate"
                activate_response = requests.post(activate_url, headers=headers)
                if activate_response.status_code == 200:
                    logging.info("âœ… active workflow æˆåŠŸ")
                else:
                    log_error(f"âš ï¸ active workflow workflow_id = {workflow_id} å¤±æ•—")
            else:
                log_error(f"âš ï¸ å‰µå»º workflow å¤±æ•—")
        
        return workflow_ids

    except Exception as e:
        log_error(f"å‰µå»º workflow éŒ¯èª¤ï¼š{e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ dify â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def dify_setup_owner():
    """
    è¨­å®š owner è³‡è¨Š
    """    
    payload = {
        "email": DIFY_EMAIL,
        "name": DIFY_NAME,
        "password": DIFY_PASSWORD
    }

    try:
        response = requests.post(DIFY_SETUP_URL, json=payload)
        response.raise_for_status()
        result = response.json()

        if result.get("result") == "success":
            logging.info("âœ… è¨»å†ŠæˆåŠŸ")
        else:
            log_error("âŒ è¨»å†Šå¤±æ•—")

    except Exception as e:
        log_error(f"è¨»å†ŠéŒ¯èª¤ï¼š{e}")

def dify_login_and_get_token() -> str:
    """
    ç™»å…¥ä¸¦å–å¾— access_token
    """
    payload = {
        "email": DIFY_EMAIL,
        "password": DIFY_PASSWORD,
        "language": "zh-Hant",
        "remember_me": True
    }

    try:
        response = requests.post(DIFY_LOGIN_URL, json=payload)
        response.raise_for_status()
        result = response.json()

        if result.get("result") == "success":
            token = result["data"]["access_token"]
            logging.info("âœ… æˆåŠŸç™»å…¥")
            logging.info("âœ… æˆåŠŸå–å¾— access token")
            return token
        else:
            log_error("âŒ ç™»å…¥å¤±æ•—")

    except Exception as e:
        log_error(f"ç™»å…¥éŒ¯èª¤ï¼š{e}")

def yaml_to_payload() -> YamlPayload:
    """
    æ ¹æ“š DIFY_TAG å°‹æ‰¾ /n8n-version/{DIFY_TAG}/ çš„æ‰€æœ‰ YMAL æª”æ¡ˆï¼Œä¸¦å°‡å…§å®¹åµŒå…¥ YAML payload ä¸­ã€‚
    """
    try:
        yaml_file = None
        yaml_dir = os.path.join(os.getcwd(), 'dify-version', DIFY_TAG)

        for filename in os.listdir(yaml_dir):
            if filename.endswith(('.yml', '.yaml')):
                yaml_file = os.path.join(yaml_dir, filename)
                with open(yaml_file, "r", encoding="utf-8") as f:
                    yaml_content = f.read()

        payload = {
            "mode": "yaml-content",
            "yaml_content": yaml_content
        }

        logging.info("âœ… ç²å– yaml æˆåŠŸ")
        return payload

    except Exception as e:
        log_error(f"ç²å– yaml éŒ¯èª¤ï¼š{e}")

def dify_create_workflow(payload, token) -> str:
    """
    ç™¼é€å‰µå»º workflow çš„è«‹æ±‚ï¼Œå›å‚³ app_id
    """
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}"
        }
        response = requests.post(DIFY_IMPORT_URL, json=payload, headers=headers)

        if response.status_code == 200:
            logging.info("âœ… å‰µå»º workflow æˆåŠŸ")
            data = response.json()
            return data.get("app_id")
        else:
            log_error("âš ï¸ å‰µå»º workflow å¤±æ•—")

    except Exception as e:
        log_error(f"å‰µå»º workflow éŒ¯èª¤ï¼š{e}")

def get_workflow_token(app_id, token) -> str:
    """
    é€é app_id æ‹¿åˆ° workflow çš„ token
    """
    url = f"{API_KEY_BASE}/{app_id}/api-keys"
    headers = {
        "Authorization": f"Bearer {token}"
    }

    try:
        response = requests.post(url, headers=headers)
        response.raise_for_status()
        data = response.json()
        logging.info("âœ… å–å¾— workflow token æˆåŠŸ")
        return data["token"]
    except Exception as e:
        log_error(f"å–å¾— workflow token éŒ¯èª¤ï¼š{e}")

def update_backend_api_key_base(new_api_key_base):
    try:
        env_file = os.path.join(os.getcwd(), 'Backend', '.env')

        if not os.path.exists(env_file):
            raise FileNotFoundError(f".env æª”æ¡ˆä¸å­˜åœ¨æ–¼ï¼š{env_file}")
        
        # ç¢ºä¿ .env æª”æ¡ˆçµå°¾æœ‰æ›è¡Œç¬¦è™Ÿ
        with open(env_file, 'a+', encoding='utf-8') as f:
            f.seek(0, os.SEEK_END)   # åˆ°æª”æ¡ˆæœ€å¾Œ
            f.seek(f.tell() - 1, os.SEEK_SET)  # ç§»åˆ°æœ€å¾Œä¸€å€‹å­—å…ƒ
            last_char = f.read()
            if last_char != '\n':
                f.write('\n')

        # æ›´æ–°æˆ–æ–°å¢ DIFY_API_KEY
        set_key(env_file, "DIFY_API_KEY", new_api_key_base)
        logging.info(f"âœ… æ›´æ–° DIFY_API_KEY æˆåŠŸ")

    except Exception as e:
        log_error(f"æ›´æ–° DIFY_API_KEY éŒ¯èª¤ï¼š{e}")

def add_model_vendor(payload, token):
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}"
        }
        response = requests.post(DIFY_ADD_MODELS_VENDOR, json=payload, headers=headers)

        if response.status_code == 200:
            logging.info("âœ… å®‰è£æ¨¡å‹ä¾›æ‡‰å•†æˆåŠŸ")
        else:
            log_error("âš ï¸ å®‰è£æ¨¡å‹ä¾›æ‡‰å•†å¤±æ•—")

    except Exception as e:
        log_error(f"å®‰è£æ¨¡å‹ä¾›æ‡‰å•†éŒ¯èª¤ï¼š{e}")

def set_openai_api_key(token) -> bool:
    try:
        payload = {
            "config_from":"predefined-model",
            "credentials":{"openai_api_key":OPENAI_API_KEY},
            "load_balancing":{"enabled":"false","configs":[]}
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}"
        }

        deadline = time.time() + 60
        while time.time() < deadline:
            try:
                response = requests.post(DIFY_SET_OPENAI_API_KEY, json=payload, headers=headers)
                if response.status_code == 201:
                    logging.info("âœ… è¨­å®š OPENAI API KEY æˆåŠŸ")
                    return True
                
                time.sleep(1)
            except Exception as e:
                log_error(f"è¨­å®š OPENAI API KEY å¤±æ•—ï¼š{e}")
        log_error("âš ï¸ è¨­å®š OPENAI API KEY å¤±æ•—")

    except Exception as e:
        log_error(f"è¨­å®š OPENAI API KEY éŒ¯èª¤ï¼š{e}")

def upload_file(token, vectorDB) -> List[str]:
    try:
        vector_dir = Path(f"dify-version/{DIFY_TAG}/{vectorDB}")
        if not vector_dir.exists():
            raise FileNotFoundError(vector_dir)

        files_to_upload = sorted(
            list(vector_dir.glob("*.txt")) + list(vector_dir.glob("*.xlsx"))
        )
        if not files_to_upload:
            log_error("âš ï¸ æ‰¾ä¸åˆ°ä»»ä½• .txt / .xlsx æª”")

        uploaded_ids = []
        headers = {
            "Authorization": f"Bearer {token}"
        }

        for fp in files_to_upload :
            mime_type = mimetypes.guess_type(fp.name)[0] or "application/octet-stream"
            try:
                with fp.open("rb") as f:
                    files = {"file": (fp.name, f, mime_type)}
                    response = requests.post(DIFY_UPLOAD_TXT, files=files, headers=headers)

                if response.status_code == 201:
                    logging.info("âœ… ä¸Šå‚³ file æˆåŠŸ")
                    file_id = response.json()["id"]
                    uploaded_ids.append(file_id)
                else:
                    log_error("âš ï¸ ä¸Šå‚³ file å¤±æ•—")

            except Exception as e:
                log_error(f"ä¸Šå‚³ file éŒ¯èª¤ï¼š{e}")
        return uploaded_ids

    except Exception as e:
        log_error(f"ä¸Šå‚³ file éŒ¯èª¤ï¼š{e}")

def init_db(file_ids, token, vectorDB_name) -> str:
    try:
        payload = {
            "data_source":{
                "type":"upload_file",
                "info_list":{
                    "data_source_type":"upload_file",
                    "file_info_list":{"file_ids":file_ids}
                }
            },
            "indexing_technique":"high_quality",
            "process_rule":{
                "rules":{
                    "pre_processing_rules":[{
                        "id":"remove_extra_spaces","enabled":"true"},
                        {"id":"remove_urls_emails","enabled":"false"}
                    ],
                    "segmentation":{"separator":"\n\n\n","max_tokens":4000,"chunk_overlap":50}
                },
                "mode":"custom"},
                "doc_form":"text_model",
                "doc_language":"English",
                "retrieval_model":{
                    "search_method":"semantic_search",
                    "reranking_enable":"false",
                    "reranking_model":{"reranking_provider_name":"","reranking_model_name":""},
                    "top_k":1,"score_threshold_enabled":"false",
                    "score_threshold":0.5,"reranking_mode":"weighted_score",
                    "weights":{
                        "weight_type":"customized",
                        "vector_setting":{
                            "vector_weight":0.7,
                            "embedding_provider_name":"","embedding_model_name":""
                        },
                        "keyword_setting":{"keyword_weight":0.3}
                    }
                },
            "embedding_model":"text-embedding-3-small",
            "embedding_model_provider":"langgenius/openai/openai"
        }
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}"
        }
        response = requests.post(DIFY_INIT_DB, json=payload, headers=headers)

        if response.status_code != 200:
            log_error("âš ï¸ è¨­å®šè³‡æ–™åº«å¤±æ•—")
        
        ds_id  = response.json()["dataset"]["id"]
        logging.info("âœ… è¨­å®šè³‡æ–™åº«æˆåŠŸ")

        # ---------- rename ----------
        rename_body = {"name": vectorDB_name}
        rename_url  = f"{DIFY_BASE}/datasets/{ds_id}"
        rp = requests.patch(rename_url, json=rename_body, headers=headers)

        if rp.status_code == 200:
            logging.info(f"âœ… åç¨±å·²æ”¹ç‚º {vectorDB_name}")
        else:
            log_error("âš ï¸ è³‡æ–™åº«æ”¹åå¤±æ•—")

        return ds_id

    except Exception as e:
        log_error(f"è¨­å®šè³‡æ–™åº«éŒ¯èª¤ï¼š{e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»è¦æ­¥é©Ÿå°è£æˆå‡½å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def step_n8n():
    def step_n8n_setup_container():
        run_shell_script("remove_n8n.sh")
        run_shell_script("run_n8n.sh")
        wait_for_container_ready(["n8n"], timeout=50, require_healthy=False)

    def step_n8n_get_api_key():
        n8n_setup_owner()
        session = n8n_login()
        global N8N_API_KEY
        N8N_API_KEY = n8n_get_api_key(session)

    def step_n8n_init_workflow():
        payloads = json_to_payload()
        n8n_create_workflow(payloads)

    with step_timer("step_n8n_setup_container"):
        _run_with_retry(step_n8n_setup_container)

    with step_timer("step_n8n_get_api_key"):
        _run_with_retry(step_n8n_get_api_key)

    with step_timer("step_n8n_init_workflow"):
        _run_with_retry(step_n8n_init_workflow)

def step_dify():
    def step_dify_setup_container():
        run_shell_script("remove_dify.sh")
        run_shell_script("run_dify.sh")
        wait_for_container_ready(DIFY_CONTAINERS, timeout=50, require_healthy=False)
        wait_for_container_ready(DIFY_CONTAINERS_HEALTHY, timeout=300, require_healthy=True)
    
    def step_dify_setup_owner():
        dify_setup_owner()
        global DIFY_TOKEN
        DIFY_TOKEN = dify_login_and_get_token()

    def step_dify_init_workflow():
        dify_payload = yaml_to_payload()
        app_id = dify_create_workflow(dify_payload, DIFY_TOKEN)
        workflow_token = get_workflow_token(app_id, DIFY_TOKEN)
        update_backend_api_key_base(workflow_token)
        
    def step_dify_init_db():
        payload = {
            "plugin_unique_identifiers": [
                "langgenius/openai:0.0.26@c1e643ac6a7732f6333a783320b4d3026fa5e31d8e7026375b98d44418d33f26"
            ]
        }
        try:
            add_model_vendor(payload, DIFY_TOKEN)
        except Exception as e:
            logging.warning(f"âš ï¸ add_model_vendor å¤±æ•—ï¼Œè·³é DB åˆå§‹åŒ–ï¼š{e}")
            return            # ç›´æ¥çµæŸæœ¬å‡½å¼ï¼Œä¸»ç¨‹å¼ç…§å¸¸åŸ·è¡Œ
        with step_timer("dify_set_openai_api_key"):
            _run_with_retry(set_openai_api_key, DIFY_TOKEN)

        file_ids = upload_file(DIFY_TOKEN, vectorDB="vectorDB1")
        init_db(file_ids, DIFY_TOKEN, vectorDB_name="scenario template")

        file_ids = upload_file(DIFY_TOKEN, vectorDB="vectorDB2")
        init_db(file_ids, DIFY_TOKEN, vectorDB_name="intent template")

    with step_timer("dify_setup_container"):
        _run_with_retry(step_dify_setup_container)

    with step_timer("dify_setup_owner"):
        _run_with_retry(step_dify_setup_owner)

    with step_timer("dify_init_workflow"):
        _run_with_retry(step_dify_init_workflow)

    with step_timer("dify_init_db"):
        step_dify_init_db()

def step_backend():
    with step_timer("backend_init"):
        run_shell_script("run_backend.sh")
        wait_for_container_ready(BACKEND_CONTAINERS, timeout=50, require_healthy=False)
        
def step_dashboard():
    with step_timer("dashboard_init"):
        run_shell_script("run_dashboard.sh")
        wait_for_container_ready(["itri-intent-dashboard"], timeout=50, require_healthy=False)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä¸»ç¨‹å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    if N8N_EXIST == "NO":
        with ThreadPoolExecutor(max_workers=2) as pool:
            futs = [pool.submit(step_dify), pool.submit(step_n8n)]
            for f in as_completed(futs): f.result()
    elif N8N_EXIST == "YES":
        step_dify()
    else:
        log_error("âš ï¸ è«‹è¨­å®š .env N8N_EXIST ç‚º YES/NO")

    ensure_docker_network()
    with ThreadPoolExecutor(max_workers=2) as pool:
        futs = [pool.submit(step_backend), pool.submit(step_dashboard)]
        for f in as_completed(futs): f.result()
        
    logging.info("ğŸ‰ éƒ¨å±¬å…¨éƒ¨æˆåŠŸï¼")